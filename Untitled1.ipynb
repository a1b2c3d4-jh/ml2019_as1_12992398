{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a1b2c3d4-jh/ml2019_as1_12992398/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK-kjCuyZZR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbk2xCDSZZvH",
        "colab_type": "text"
      },
      "source": [
        "Review Report on \"[LEC98] Convolutional Neural Network\"\n",
        "GitHub Notebook file link:\n",
        "Introduction\n",
        "With the information technology development these years, convolutional neural netword has been widely applied in artificial intelligence. This report is about “Gradient Based Learning Applied to Document Recognition”, which I choose to help understand the applications of convolutional neural network.\n",
        "\n",
        "Content\n",
        "This article is about an automatic learning-based recognition system of composite model combination for complex text recognition, written by lencun et al. At the same time, this paper introduces the difference between this model and the traditional heuristic method designed manually and the possibility of replacing it. Lencun began his research by recognizing handwritten versions of paper. Instead of using traditional manual methods to find features, they first grab important features from pixel-level images. At the same time, Lencun believes that the parameters of convolution network are much less than those of the fully constructed neural network and can guarantee the dimension parameters of the input image. So Lencun introduced a convolutional neural network lenet-5 (Szegedy et al. 2015). The network consists of the first C1 convolution layer, the lower sub-sampling layer, one convolution layer, the second sub-sampling layer and the last two convolution and pool layers. The first convolution layer consists of six feature maps, each of which allows the filter to filter 28 times 28 and 5 times 5. Then the feature mapping specification of the next layer is reduced to 14 times 14 pixels. The third layer of convolution continues to shrink to 10 times 10 pixels. At the S4 sampling layer, only 16 feature pixel maps with size 5 times 5 are available. Then S4 uses a total of 150 feature maps and C5 to connect perfectly, and then forms two fully joined layers with 84 and 10 units. Next, in order to get the optimal shape recognition algorithm, Lenkun et al. take the single number recognition problem as the criterion. They created a handwritten digital database called mnist. Train on it and test different recognition methods. The comparison shows that the lowest error rate of the enhanced lenet-4 in the test set is 0.7%, followed by lenet-5 and V-SVM deployment 9, which is 0.8%. In addition, Lencn also found that neural network methods require fewer samples than S VM and K-NN. These methods may require more time for data training, but they can be ignored. Finally, Lekcun uses the method of graph transformation network to solve the problem of accuracy in the process of text recognition. This method identifies shape data instead of fixed size quantification. At the same time, the whole system uses gradient method to optimize the global variables, in which LeNet technology is the core.\n",
        "Innovation\n",
        "\n",
        "I believe that for Deep learning development, the research work in the paper would be one of the most grateful innovations. According to Szegedy et al., LeNet-5 architecture was introduced by LenCun et al., which become a widely-used model of CNN. Besides, they also introduced MNIST database, which is now a benchmark of testing a new learning technique. (Simard et al. 2003). Moreover, another prevalent model for image recognition was discussed in the paper, Graph Transformer Networks (GTN), is based on gradient learning methods. \n",
        "LeNet-5 architecture\n",
        "In accordance to Brownlee, LeNet-5 can be regarded as the first successful and vital application of CNNs’. In general, LeNet-5 is designed to combine a convolutional layer and a pooling layer which is subsampling layer in the paper. In addition, while designing a CNNs for image recognition, the pattern of repetitive blocks of convolutional layer and pooling layers becomes popular. For instance, the Lenet-5 uses a fixed-size image as input, which is also a brilliant idea of this innovation. Moreover, another new pattern of using more quantity of filters as the network was developed by Lenet-5, however, the quantity of filters of Lenet-5 is still much less than the currently common-used networks, such as VGG, ResNet as well as GoogleNet. \n",
        "Although the ResNet, GoogleNet and other complicated convolutional neural networks that are with high capacity and better performance have entirely replace Lenet-5, it can be argued. Personally speaking, as one of the most crucial innovation of Deep Learning development, its patterns and techniques facilitate the designs of new learning methods. \n",
        "GTN design\n",
        "There is also another vital innovation of the paper, which is Graph Transformer Networks(GTN). It has multiple differentiated models rather than hand-crafted methods. In addition, it utilizes graphs for information storage. Most importantly, the gradient learning methods are used to optimize the measurement of global performance. Moreover, both gradient descent and backpropagation techniques are applied to Graph Transformer Networks.\n",
        "Technical quality\n",
        "\n",
        "In my opinion, this document has a very good technical quality. It describes the Lenet model in great detail and makes repeated comparisons. This article outlines the process of building lenet-5 model for readers. Readers can follow the demonstration steps in the article to build the model of lenet-5. I used pycharm platform to build the model network and carried out 28 training sets. The process lost about 0.31%, ranging from 0.3% to 0.35%, does not exclude that I personally reduced the gradient. This paper explains in detail how to use convolutional neural network to recognize handwritten version numbers. In addition, this paper compares the convolutional neural network with the handwritten digital test set, without training. From the error probability, the required memory capacity and the running time, we can get a best place method. From this we can see that the method is reliable.\n",
        "Application and X-factor\n",
        "\n",
        "With the development of artificial intelligence and mechanical learning, document recognition can be solved by more and more methods. In recent years, Google lenet, RESNET and many deep CNNs have gradually replaced Lenet-5 (Szegedy et al. 2015). These methods are widely used in classifier selection and target detection. However, because of the excessive data required and the complexity of the algorithm, it is not suitable for the document recognition problem to be solved in this paper. From this point of view, Lenet-5 is the most suitable technology for document recognition, and many object classification problems can be easily solved by this method. According to the trend described in this paper, in the near future, the use of Letnet-5 will gradually disappear and be replaced by other more advanced methods, but its role in the development of deep learning can not be ignored, especially the enlightening role of CNN design. To sum up, the literature has promoted the development of CNN and gradient learning.\n",
        "Presentation\n",
        "In my opinion, this paper is of high quality in terms of presentation style, clarity or depth of argument. This paper skillfully uses statistics, flow charts, linear graphs and histograms to help readers understand the text content and elaborate LENET-5 and GTN technologies. The whole paper is divided into ten parts, the first sentence of each part has profound argument, but I think I have room for improvement, for example, to simplify the description of GTN development process, which can be more accessible to readers.\n",
        "\n",
        "[1] Bottou, L. & LeCun, Y. 2005, 'Graph Transformer Networks for Image Recognition', http://yann.lecun.com/exdb/publis/pdf/bottou-05.pdf\n",
        "[2] The Verge 2019, ‘Godfathers of AI’ honored with Turing Award, the Nobel Prize of computing, viewed 26 August 2019, https://www.theverge.com/2019/3/27/18280665/ai-godfathers-turing-award-2018-yoshua-bengio-geoffrey-hinton-yann-lecun \n",
        "[3] Towards Data Science 2019, Illustrated: 10 CNN Architectures, viewed 26 August 2019, https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d#e276\n"
      ]
    }
  ]
}